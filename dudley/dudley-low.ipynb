{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b0bfb3-96d6-48cc-b261-6ed2924aad11",
   "metadata": {},
   "source": [
    "# Dudley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98425636-8a8b-421f-afd8-68dccc65159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9da0bc-1413-4bcd-8b51-9286d86fdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"amalgamated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9cd48-db18-40fa-bbdc-d20473815d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = [\n",
    "    \"BTC-USD_High\",\n",
    "    \"BTC-USD_Low\",\n",
    "    \"AdrBalNtv0.01Cnt\",\n",
    "    \"AdrBalNtv0.1Cnt\",\n",
    "    \"AdrBalNtv1Cnt\",\n",
    "    \"AdrBalNtv10Cnt\",\n",
    "    \"BlkSizeMeanByte\",\n",
    "    \"CapRealUSD\",\n",
    "    \"FeeByteMeanNtv\",\n",
    "    \"FlowInExNtv\",\n",
    "    \"FlowOutExNtv\",\n",
    "    \"FlowTfrFromExCnt\",\n",
    "    \"HashRate\",\n",
    "    \"NDF\",\n",
    "    \"SplyAct1d\",\n",
    "    \"SplyActPct1yr\",\n",
    "    \"TxCnt\",\n",
    "    \"VelCur1yr\",\n",
    "    'SPY_High',\n",
    "    'SPY_Low',\n",
    "    'QQQ_High',\n",
    "    'QQQ_Low',\n",
    "    '^IRX_High',\n",
    "    '^IRX_Low',\n",
    "    '^TNX_High',\n",
    "    '^TNX_Low',\n",
    "    '^TYX_High',\n",
    "    '^TYX_Low',\n",
    "    'Global_Liquidity_Index',\n",
    "    'BTC-USD_High_SMA_5',\n",
    "    'BTC-USD_Low_SMA_5',\n",
    "    'BTC-USD_High_SMA_10',\n",
    "    'BTC-USD_Low_SMA_10',\n",
    "    'BTC-USD_High_SMA_20',\n",
    "    'BTC-USD_Low_SMA_20',\n",
    "    'BTC-USD_High_SMA_50',\n",
    "    'BTC-USD_Low_SMA_50',\n",
    "    'BTC-USD_High_SMA_100',\n",
    "    'BTC-USD_Low_SMA_100'\n",
    "]\n",
    "\n",
    "df = df[vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5001d85-6388-4d46-b27b-36b8d6ae11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = df.diff()\n",
    "\n",
    "# The first row will be NaN because there's no previous data to subtract from the first entry\n",
    "# If you wish to remove the NaN values, you can drop the first row\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1f40562-edc0-4a3a-94cf-d5a16f17ead5",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df is your DataFrame and 'BTC-USD_High' is the column of interest\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['BTC-USD_High'], bins=30, edgecolor='black')\n",
    "plt.title('Distribution of BTC-USD High Values - Histogram')\n",
    "plt.xlabel('BTC-USD High')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# KDE Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(df['BTC-USD_High'], fill=True)\n",
    "plt.title('Distribution of BTC-USD High Values - KDE Plot')\n",
    "plt.xlabel('BTC-USD High')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ae61c-85c4-4789-93c6-e473fb853a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.transform.feature import MissingValueImputation\n",
    "\n",
    "# Assuming df is your DataFrame with the data\n",
    "target_column = 'BTC-USD_Low'  # Replace with your target column name\n",
    "\n",
    "# Ensure the DataFrame's index is a datetime index and set the frequency explicitly if needed\n",
    "df.index = pd.to_datetime(df.index)\n",
    "freq = \"D\"  # Set the frequency of your data, e.g., 'D' for daily. Adjust as needed.\n",
    "df = df.asfreq(freq)\n",
    "\n",
    "# Define the prediction length\n",
    "prediction_length = 40  # Set your prediction length\n",
    "\n",
    "# Select dynamic features from the DataFrame, excluding the target column\n",
    "dynamic_feature_columns = df.columns.drop(target_column)  # This excludes the target column\n",
    "\n",
    "# Extract dynamic features as a numpy array\n",
    "dynamic_features = df[dynamic_feature_columns].values.transpose()\n",
    "\n",
    "# Assuming all dynamic features are known in the future, adjust the dimensions accordingly\n",
    "dynamic_dims = [1] * len(dynamic_feature_columns)  # Adjust based on actual dynamic features\n",
    "\n",
    "# Make sure the lengths match when creating ListDataset\n",
    "training_data = ListDataset([\n",
    "    {\n",
    "        \"start\": df.index[0],\n",
    "        \"target\": df[target_column][:-prediction_length],\n",
    "        \"feat_dynamic_real\": dynamic_features[:, :-prediction_length]\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Adjust the slicing for dynamic features for the test dataset to ensure correct dimensions\n",
    "test_dynamic_features_sliced = dynamic_features[:, -prediction_length*2:]\n",
    "\n",
    "test_data = ListDataset([\n",
    "    {\n",
    "        \"start\": df.index[-prediction_length],\n",
    "        \"target\": df[target_column][-prediction_length:].values,\n",
    "        \"feat_dynamic_real\": test_dynamic_features_sliced\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer Estimator\n",
    "estimator = TemporalFusionTransformerEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=3*prediction_length,  # Optional: adjust based on your needs\n",
    "    num_heads=8,\n",
    "    hidden_dim=64,\n",
    "    variable_dim=64,\n",
    "    dynamic_dims=dynamic_dims,\n",
    "    quantiles=[0.1, 0.5, 0.9],  # Specifying the quantiles for forecasting\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-08,\n",
    "    dropout_rate=0.1,\n",
    "    patience=10,\n",
    "    batch_size=64,\n",
    "    num_batches_per_epoch=100,\n",
    "    trainer_kwargs={'max_epochs': 160},  # Adjust 'gpus' based on your setup\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "predictor = estimator.train(training_data)\n",
    "\n",
    "# Debugging: Print dimensions to verify alignment\n",
    "print(f\"Dynamic Features Training Shape: {training_dynamic_features.shape}\")\n",
    "print(f\"Dynamic Features Test Shape: {test_dynamic_features_sliced.shape}\")\n",
    "\n",
    "# Collect actual and predicted values for evaluation, including percentiles\n",
    "actuals = df[target_column][-prediction_length:].values\n",
    "mean_predictions = []\n",
    "p10_predictions = []\n",
    "p50_predictions = []\n",
    "p90_predictions = []\n",
    "\n",
    "for forecast in predictor.predict(test_data):\n",
    "    mean_predictions.append(forecast.mean)\n",
    "    p10_predictions.append(forecast.quantile(0.1))\n",
    "    p50_predictions.append(forecast.quantile(0.5))  # Median\n",
    "    p90_predictions.append(forecast.quantile(0.9))\n",
    "\n",
    "# Convert lists to numpy arrays for slicing\n",
    "mean_predictions = np.array(mean_predictions).flatten()[:prediction_length]\n",
    "p10_predictions = np.array(p10_predictions).flatten()[:prediction_length]\n",
    "p50_predictions = np.array(p50_predictions).flatten()[:prediction_length]\n",
    "p90_predictions = np.array(p90_predictions).flatten()[:prediction_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edfd4f-da3d-4ba5-ac72-637c722c6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure forecast_start_date is a datetime object and exists in df.index\n",
    "forecast_start_date = end_training + pd.Timedelta(days=1)\n",
    "\n",
    "# Plot the forecast and actual values starting from the forecast start date\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot actual values from the start of the forecast\n",
    "actuals_start_index = df.index.get_loc(forecast_start_date)\n",
    "plt.plot(df.index[actuals_start_index:], df[target_column][actuals_start_index:], label=\"True values\", color=\"black\")\n",
    "\n",
    "# Plot forecast values\n",
    "forecast_index = pd.date_range(start=forecast_start_date, periods=prediction_length, freq=freq)\n",
    "plt.plot(forecast_index, mean_predictions, color='red', linestyle='--', label=\"Forecast (mean)\")\n",
    "plt.fill_between(forecast_index, p10_predictions, p90_predictions, color='red', alpha=0.3, label=\"P10-P90 interval\")\n",
    "plt.fill_between(forecast_index, p10_predictions, p50_predictions, color='red', alpha=0.5, label=\"P10-P50 interval\")\n",
    "plt.fill_between(forecast_index, p50_predictions, p90_predictions, color='red', alpha=0.5, label=\"P50-P90 interval\")\n",
    "\n",
    "# Add a vertical line and other plot elements\n",
    "plt.axvline(x=forecast_start_date, color='blue', linestyle='--', label='Start of forecast')\n",
    "plt.legend(loc=\"upper left\", fontsize=\"large\")\n",
    "plt.title('Forecast vs Actual Values from Forecast Start')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fd538-c47b-4b5a-9131-a6f5995b01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sMAPE\n",
    "def calculate_smape(forecasts, actuals):\n",
    "    return 100 * np.mean(2 * np.abs(forecasts - actuals) / (np.abs(actuals) + np.abs(forecasts)))\n",
    "\n",
    "# Calculate standard evaluation metrics for mean predictions\n",
    "mae = np.mean(np.abs(mean_predictions - actuals))\n",
    "rmse = np.sqrt(np.mean(np.square(mean_predictions - actuals)))\n",
    "mape = np.mean(np.abs((mean_predictions - actuals) / actuals)) * 100\n",
    "smape = calculate_smape(mean_predictions, actuals)\n",
    "\n",
    "# Calculate the percentage of actuals within the 10th to 90th percentile range\n",
    "within_range = np.sum((actuals >= p10_predictions) & (actuals <= p90_predictions)) / len(actuals) * 100\n",
    "\n",
    "print(\"Evaluation Metrics for Mean Predictions:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"sMAPE: {smape:.2f}%\")\n",
    "print(f\"Percentage of Actuals within P10-P90 Interval: {within_range:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b4776-af52-429f-9022-99e9326b027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcc_share = 7111.9523\n",
    "\n",
    "# Adjust predictions and actuals by dividing by btcc_share\n",
    "adjusted_mean_predictions = mean_predictions / btcc_share\n",
    "adjusted_p10_predictions = p10_predictions / btcc_share\n",
    "adjusted_p50_predictions = p50_predictions / btcc_share\n",
    "adjusted_p90_predictions = p90_predictions / btcc_share\n",
    "adjusted_actuals = actuals / btcc_share\n",
    "\n",
    "# Calculate standard evaluation metrics for adjusted mean predictions\n",
    "mae = np.mean(np.abs(adjusted_mean_predictions - adjusted_actuals))\n",
    "rmse = np.sqrt(np.mean(np.square(adjusted_mean_predictions - adjusted_actuals)))\n",
    "mape = np.mean(np.abs((adjusted_mean_predictions - adjusted_actuals) / adjusted_actuals)) * 100\n",
    "adjusted_smape = calculate_smape(adjusted_mean_predictions, adjusted_actuals)\n",
    "\n",
    "# Calculate the percentage of adjusted actuals within the 10th to 90th percentile range\n",
    "within_range = np.sum((adjusted_actuals >= adjusted_p10_predictions) & (adjusted_actuals <= adjusted_p90_predictions)) / len(adjusted_actuals) * 100\n",
    "\n",
    "print(\"Evaluation Metrics for Adjusted Mean Predictions:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"sMAPE: {adjusted_smape:.2f}%\")\n",
    "print(f\"Percentage of Adjusted Actuals within P10-P90 Interval: {within_range:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa71034-caf4-433e-ae72-05fa5395ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define a path to save the model\n",
    "model_save_path = 'dudley_low.pth'\n",
    "\n",
    "# Assuming 'predictor' is the trained model from DeepAREstimator\n",
    "torch.save(predictor, model_save_path)\n",
    "\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
