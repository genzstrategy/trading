{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b0bfb3-96d6-48cc-b261-6ed2924aad11",
   "metadata": {},
   "source": [
    "# Ron High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98425636-8a8b-421f-afd8-68dccc65159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9da0bc-1413-4bcd-8b51-9286d86fdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"yahoo-finance.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9cd48-db18-40fa-bbdc-d20473815d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "yfinance_metrics = [\n",
    "    \"BTC-CAD_High\",\n",
    "    \"BTC-CAD_Low\",\n",
    "]\n",
    "\n",
    "df = df[yfinance_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5001d85-6388-4d46-b27b-36b8d6ae11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame after resampling and you've already dropped NA values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Add an assertion to ensure there are no NA values in the DataFrame\n",
    "assert df.isnull().sum().sum() == 0, \"DataFrame contains NA values\"\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ae61c-85c4-4789-93c6-e473fb853a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.transform.feature import MissingValueImputation\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "                                   \n",
    "# Assuming df is your DataFrame with the data\n",
    "target_column = 'BTC-CAD_High'  # Replace with your target column name\n",
    "\n",
    "# Ensure the DataFrame's index is a datetime index and set the frequency explicitly if needed\n",
    "df.index = pd.to_datetime(df.index)\n",
    "freq = \"D\"  # Set the frequency of your data, e.g., 'D' for daily. Adjust as needed.\n",
    "df = df.asfreq(freq)\n",
    "\n",
    "# Define the prediction length\n",
    "prediction_length = 1  # Set your prediction length\n",
    "\n",
    "# Make sure the lengths match when creating ListDataset\n",
    "training_data = ListDataset([\n",
    "    {\n",
    "        \"start\": df.index[0],\n",
    "        \"target\": df[target_column][:-100]    \n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "test_data = ListDataset([\n",
    "    {\n",
    "        \"start\": df.index[-100],\n",
    "        \"target\": df[target_column][-100:].values\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer Estimator\n",
    "estimator = TemporalFusionTransformerEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=60,  # Optional: adjust based on your needs\n",
    "    num_heads=32,\n",
    "    hidden_dim=1024,\n",
    "    variable_dim=1024,\n",
    "    quantiles=[0.1, 0.5, 0.9],  # Specifying the quantiles for forecasting\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-08,\n",
    "    dropout_rate=0.1,\n",
    "    patience=10,\n",
    "    batch_size=128,\n",
    "    num_batches_per_epoch=100,\n",
    "    trainer_kwargs={'max_epochs': 1000},  # Adjust 'gpus' based on your setup\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "predictor = estimator.train(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df585298-a43f-48ca-83ad-db932cd68796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect actual and predicted values for evaluation, including percentiles\n",
    "actuals = df[target_column][-prediction_length:].values\n",
    "\n",
    "mean_predictions = []\n",
    "p10_predictions = []\n",
    "p50_predictions = []\n",
    "p90_predictions = []\n",
    "\n",
    "for forecast in predictor.predict(test_data):\n",
    "    p10_predictions.append(forecast.quantile(0.1))\n",
    "    p50_predictions.append(forecast.quantile(0.5))  # Median\n",
    "    p90_predictions.append(forecast.quantile(0.9))\n",
    "\n",
    "# Convert lists to numpy arrays for slicing\n",
    "p10_predictions = np.array(p10_predictions).flatten()[:prediction_length]\n",
    "p50_predictions = np.array(p50_predictions).flatten()[:prediction_length]\n",
    "p90_predictions = np.array(p90_predictions).flatten()[:prediction_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960df258-1313-46e2-92ba-0d8b39afa15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row = df.iloc[-prediction_length:, :1]\n",
    "last_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cc2ba-d3a8-4bfe-9945-bc79f261cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p10_predictions, p50_predictions, p90_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fd538-c47b-4b5a-9131-a6f5995b01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sMAPE\n",
    "def calculate_smape(forecasts, actuals):\n",
    "    return 100 * np.mean(2 * np.abs(forecasts - actuals) / (np.abs(actuals) + np.abs(forecasts)))\n",
    "\n",
    "# Calculate standard evaluation metrics for mean predictions\n",
    "mae = np.mean(np.abs(p50_predictions - actuals))\n",
    "mape = np.mean(np.abs((p50_predictions - actuals) / actuals)) * 100\n",
    "smape = calculate_smape(p50_predictions, actuals)\n",
    "\n",
    "# Calculate the percentage of actuals within the 10th to 90th percentile range\n",
    "within_range = np.sum((actuals >= p10_predictions) & (actuals <= p90_predictions)) / len(actuals) * 100\n",
    "\n",
    "print(\"Evaluation Metrics for Mean Predictions:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"sMAPE: {smape:.2f}%\")\n",
    "print(f\"Percentage of Actuals within P10-P90 Interval: {within_range:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa71034-caf4-433e-ae72-05fa5395ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define a path to save the model\n",
    "model_save_path = 'ron_high_huge.pth'\n",
    "\n",
    "# Assuming 'predictor' is the trained model from DeepAREstimator\n",
    "torch.save(predictor, model_save_path)\n",
    "\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447bbd5-49e0-4e98-b82f-adf0ce77d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gluonts.dataset.common import ListDataset\n",
    "# [other imports]\n",
    "\n",
    "# [Assuming your DataFrame 'df' and the target_column are already defined]\n",
    "# [Also assuming the estimator and model training are already done and you have 'predictor']\n",
    "\n",
    "prediction_length = 1  # Forecasting 30 days into the future\n",
    "context_length = 60  # The length of the history to consider for the prediction\n",
    "\n",
    "# Rolling window prediction function\n",
    "def perform_rolling_prediction(df, predictor, target_column, context_length, prediction_length):\n",
    "    rolling_predictions = []\n",
    "    prediction_dates = []\n",
    "    temp_df = df.copy()  # Create a copy of the dataframe to modify\n",
    "\n",
    "    # Create dates for predictions\n",
    "    start_prediction_date = temp_df.index.max() + pd.Timedelta(1, unit='D')\n",
    "    prediction_dates = pd.date_range(start=start_prediction_date, periods=30, freq='D')\n",
    "    \n",
    "    for i, prediction_date in enumerate(prediction_dates):\n",
    "        end_idx = len(temp_df) - prediction_length + i\n",
    "        test_data = ListDataset([\n",
    "            {\n",
    "                \"start\": temp_df.index[0],\n",
    "                \"target\": temp_df[target_column][:end_idx].values\n",
    "            }\n",
    "        ], freq='D')\n",
    "\n",
    "        forecast = next(predictor.predict(test_data))\n",
    "        predicted_value = forecast.quantile(0.5)[-1]\n",
    "        rolling_predictions.append(predicted_value)\n",
    "        \n",
    "        # Append the predicted value for future rolling windows\n",
    "        new_row = pd.DataFrame({target_column: [predicted_value]}, index=[prediction_date])\n",
    "        temp_df = pd.concat([temp_df, new_row])\n",
    "    \n",
    "    return prediction_dates, rolling_predictions\n",
    "\n",
    "# Perform rolling predictions for the next 30 days\n",
    "prediction_dates, rolling_predictions = perform_rolling_prediction(df, predictor, target_column, context_length, prediction_length)\n",
    "\n",
    "# Print the rolling predictions\n",
    "print(\"Rolling Predictions for the next 30 days:\")\n",
    "for date, prediction in zip(prediction_dates, rolling_predictions):\n",
    "    print(f\"{date.strftime('%Y-%m-%d')}: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
