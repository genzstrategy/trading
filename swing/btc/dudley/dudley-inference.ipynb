{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b0bfb3-96d6-48cc-b261-6ed2924aad11",
   "metadata": {},
   "source": [
    "# Dudley Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98425636-8a8b-421f-afd8-68dccc65159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9da0bc-1413-4bcd-8b51-9286d86fdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"amalgamated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c9cd48-db18-40fa-bbdc-d20473815d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = [\n",
    "    \"BTC-USD_High\",\n",
    "    \"BTC-USD_Low\",\n",
    "    \"AdrBalNtv0.01Cnt\",\n",
    "    \"AdrBalNtv0.1Cnt\",\n",
    "    \"AdrBalNtv1Cnt\",\n",
    "    \"AdrBalNtv10Cnt\",\n",
    "    \"BlkSizeMeanByte\",\n",
    "    \"CapRealUSD\",\n",
    "    \"FeeByteMeanNtv\",\n",
    "    \"FlowInExNtv\",\n",
    "    \"FlowOutExNtv\",\n",
    "    \"FlowTfrFromExCnt\",\n",
    "    \"HashRate\",\n",
    "    \"NDF\",\n",
    "    \"SplyAct1d\",\n",
    "    \"SplyActPct1yr\",\n",
    "    \"TxCnt\",\n",
    "    \"VelCur1yr\",\n",
    "    'SPY_High',\n",
    "    'SPY_Low',\n",
    "    'QQQ_High',\n",
    "    'QQQ_Low',\n",
    "    '^IRX_High',\n",
    "    '^IRX_Low',\n",
    "    '^TNX_High',\n",
    "    '^TNX_Low',\n",
    "    '^TYX_High',\n",
    "    '^TYX_Low',\n",
    "    'Global_Liquidity_Index',\n",
    "    'BTC-USD_High_SMA_5',\n",
    "    'BTC-USD_Low_SMA_5',\n",
    "    'BTC-USD_High_SMA_10',\n",
    "    'BTC-USD_Low_SMA_10',\n",
    "    'BTC-USD_High_SMA_20',\n",
    "    'BTC-USD_Low_SMA_20',\n",
    "    'BTC-USD_High_SMA_50',\n",
    "    'BTC-USD_Low_SMA_50',\n",
    "    'BTC-USD_High_SMA_100',\n",
    "    'BTC-USD_Low_SMA_100'\n",
    "]\n",
    "\n",
    "df = df[vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5001d85-6388-4d46-b27b-36b8d6ae11d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC-USD_High</th>\n",
       "      <th>BTC-USD_Low</th>\n",
       "      <th>AdrBalNtv0.01Cnt</th>\n",
       "      <th>AdrBalNtv0.1Cnt</th>\n",
       "      <th>AdrBalNtv1Cnt</th>\n",
       "      <th>AdrBalNtv10Cnt</th>\n",
       "      <th>BlkSizeMeanByte</th>\n",
       "      <th>CapRealUSD</th>\n",
       "      <th>FeeByteMeanNtv</th>\n",
       "      <th>FlowInExNtv</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC-USD_High_SMA_5</th>\n",
       "      <th>BTC-USD_Low_SMA_5</th>\n",
       "      <th>BTC-USD_High_SMA_10</th>\n",
       "      <th>BTC-USD_Low_SMA_10</th>\n",
       "      <th>BTC-USD_High_SMA_20</th>\n",
       "      <th>BTC-USD_Low_SMA_20</th>\n",
       "      <th>BTC-USD_High_SMA_50</th>\n",
       "      <th>BTC-USD_Low_SMA_50</th>\n",
       "      <th>BTC-USD_High_SMA_100</th>\n",
       "      <th>BTC-USD_Low_SMA_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-06-16</th>\n",
       "      <td>773.721985</td>\n",
       "      <td>696.523010</td>\n",
       "      <td>2707487.0</td>\n",
       "      <td>1194210.0</td>\n",
       "      <td>486235.0</td>\n",
       "      <td>138918.0</td>\n",
       "      <td>9.576345e+05</td>\n",
       "      <td>5.504034e+09</td>\n",
       "      <td>5.800000e-07</td>\n",
       "      <td>103461.872142</td>\n",
       "      <td>...</td>\n",
       "      <td>715.075403</td>\n",
       "      <td>660.682800</td>\n",
       "      <td>651.692102</td>\n",
       "      <td>616.700800</td>\n",
       "      <td>605.640051</td>\n",
       "      <td>574.958147</td>\n",
       "      <td>514.898219</td>\n",
       "      <td>498.599160</td>\n",
       "      <td>471.263739</td>\n",
       "      <td>460.382540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-17</th>\n",
       "      <td>775.356018</td>\n",
       "      <td>716.556030</td>\n",
       "      <td>2697438.0</td>\n",
       "      <td>1194425.0</td>\n",
       "      <td>485785.0</td>\n",
       "      <td>138762.0</td>\n",
       "      <td>9.202532e+05</td>\n",
       "      <td>5.528068e+09</td>\n",
       "      <td>5.700000e-07</td>\n",
       "      <td>101196.210866</td>\n",
       "      <td>...</td>\n",
       "      <td>733.177808</td>\n",
       "      <td>682.586206</td>\n",
       "      <td>670.201807</td>\n",
       "      <td>631.605005</td>\n",
       "      <td>617.734201</td>\n",
       "      <td>587.150998</td>\n",
       "      <td>521.414319</td>\n",
       "      <td>504.197281</td>\n",
       "      <td>474.856979</td>\n",
       "      <td>463.432041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-18</th>\n",
       "      <td>777.989990</td>\n",
       "      <td>733.929016</td>\n",
       "      <td>2700495.0</td>\n",
       "      <td>1195493.0</td>\n",
       "      <td>486147.0</td>\n",
       "      <td>138745.0</td>\n",
       "      <td>8.705890e+05</td>\n",
       "      <td>5.549665e+09</td>\n",
       "      <td>5.200000e-07</td>\n",
       "      <td>71224.258522</td>\n",
       "      <td>...</td>\n",
       "      <td>745.575000</td>\n",
       "      <td>696.474609</td>\n",
       "      <td>689.716907</td>\n",
       "      <td>647.684906</td>\n",
       "      <td>628.935699</td>\n",
       "      <td>598.238498</td>\n",
       "      <td>527.866439</td>\n",
       "      <td>509.955521</td>\n",
       "      <td>478.461759</td>\n",
       "      <td>466.638821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-19</th>\n",
       "      <td>766.620972</td>\n",
       "      <td>745.627991</td>\n",
       "      <td>2698863.0</td>\n",
       "      <td>1193758.0</td>\n",
       "      <td>486101.0</td>\n",
       "      <td>138807.0</td>\n",
       "      <td>7.119917e+05</td>\n",
       "      <td>5.566891e+09</td>\n",
       "      <td>4.900000e-07</td>\n",
       "      <td>44574.581631</td>\n",
       "      <td>...</td>\n",
       "      <td>757.998389</td>\n",
       "      <td>713.039404</td>\n",
       "      <td>708.158704</td>\n",
       "      <td>665.152606</td>\n",
       "      <td>640.049298</td>\n",
       "      <td>609.371747</td>\n",
       "      <td>534.087119</td>\n",
       "      <td>515.914141</td>\n",
       "      <td>481.888709</td>\n",
       "      <td>469.924971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-20</th>\n",
       "      <td>764.083984</td>\n",
       "      <td>732.726990</td>\n",
       "      <td>2707494.0</td>\n",
       "      <td>1197120.0</td>\n",
       "      <td>487506.0</td>\n",
       "      <td>138842.0</td>\n",
       "      <td>7.968285e+05</td>\n",
       "      <td>5.573669e+09</td>\n",
       "      <td>5.900000e-07</td>\n",
       "      <td>75586.050007</td>\n",
       "      <td>...</td>\n",
       "      <td>771.554590</td>\n",
       "      <td>725.072607</td>\n",
       "      <td>726.654401</td>\n",
       "      <td>681.092804</td>\n",
       "      <td>650.922598</td>\n",
       "      <td>619.974997</td>\n",
       "      <td>540.319218</td>\n",
       "      <td>521.610141</td>\n",
       "      <td>485.311599</td>\n",
       "      <td>473.151301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-01</th>\n",
       "      <td>43243.167969</td>\n",
       "      <td>41879.191406</td>\n",
       "      <td>12619292.0</td>\n",
       "      <td>4572236.0</td>\n",
       "      <td>1019807.0</td>\n",
       "      <td>154559.0</td>\n",
       "      <td>1.706556e+06</td>\n",
       "      <td>4.479779e+11</td>\n",
       "      <td>2.166770e-07</td>\n",
       "      <td>28549.993968</td>\n",
       "      <td>...</td>\n",
       "      <td>43380.512500</td>\n",
       "      <td>42080.950000</td>\n",
       "      <td>42217.319922</td>\n",
       "      <td>40923.807813</td>\n",
       "      <td>42447.216016</td>\n",
       "      <td>41136.506836</td>\n",
       "      <td>43648.887969</td>\n",
       "      <td>42018.851875</td>\n",
       "      <td>40943.784688</td>\n",
       "      <td>39519.390898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-02</th>\n",
       "      <td>43422.488281</td>\n",
       "      <td>42584.335938</td>\n",
       "      <td>12618955.0</td>\n",
       "      <td>4571607.0</td>\n",
       "      <td>1019919.0</td>\n",
       "      <td>154576.0</td>\n",
       "      <td>1.700535e+06</td>\n",
       "      <td>4.482718e+11</td>\n",
       "      <td>2.066230e-07</td>\n",
       "      <td>26037.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>43505.575000</td>\n",
       "      <td>42258.435156</td>\n",
       "      <td>42546.833594</td>\n",
       "      <td>41330.051953</td>\n",
       "      <td>42456.607422</td>\n",
       "      <td>41142.516406</td>\n",
       "      <td>43649.520547</td>\n",
       "      <td>42035.196797</td>\n",
       "      <td>41026.671992</td>\n",
       "      <td>39608.143164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-03</th>\n",
       "      <td>43359.941406</td>\n",
       "      <td>42890.808594</td>\n",
       "      <td>12623419.0</td>\n",
       "      <td>4572105.0</td>\n",
       "      <td>1019723.0</td>\n",
       "      <td>154548.0</td>\n",
       "      <td>1.696869e+06</td>\n",
       "      <td>4.483556e+11</td>\n",
       "      <td>7.011070e-07</td>\n",
       "      <td>14200.160578</td>\n",
       "      <td>...</td>\n",
       "      <td>43516.389844</td>\n",
       "      <td>42472.930469</td>\n",
       "      <td>42834.449219</td>\n",
       "      <td>41668.253125</td>\n",
       "      <td>42471.324609</td>\n",
       "      <td>41200.826172</td>\n",
       "      <td>43654.962891</td>\n",
       "      <td>42059.153594</td>\n",
       "      <td>41111.942305</td>\n",
       "      <td>39699.428008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-04</th>\n",
       "      <td>43097.644531</td>\n",
       "      <td>42374.832031</td>\n",
       "      <td>12612131.0</td>\n",
       "      <td>4571390.0</td>\n",
       "      <td>1019829.0</td>\n",
       "      <td>154504.0</td>\n",
       "      <td>1.576919e+06</td>\n",
       "      <td>4.484874e+11</td>\n",
       "      <td>3.948680e-07</td>\n",
       "      <td>14806.864074</td>\n",
       "      <td>...</td>\n",
       "      <td>43368.129687</td>\n",
       "      <td>42405.622656</td>\n",
       "      <td>43118.765625</td>\n",
       "      <td>41951.169922</td>\n",
       "      <td>42460.220703</td>\n",
       "      <td>41234.296875</td>\n",
       "      <td>43663.616875</td>\n",
       "      <td>42072.187969</td>\n",
       "      <td>41200.536641</td>\n",
       "      <td>39789.007461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>43494.250000</td>\n",
       "      <td>42264.816406</td>\n",
       "      <td>12605783.0</td>\n",
       "      <td>4571143.0</td>\n",
       "      <td>1019671.0</td>\n",
       "      <td>154513.0</td>\n",
       "      <td>1.415660e+06</td>\n",
       "      <td>4.487452e+11</td>\n",
       "      <td>2.925980e-07</td>\n",
       "      <td>23439.165199</td>\n",
       "      <td>...</td>\n",
       "      <td>43323.498437</td>\n",
       "      <td>42398.796875</td>\n",
       "      <td>43247.251953</td>\n",
       "      <td>42195.082422</td>\n",
       "      <td>42456.619531</td>\n",
       "      <td>41243.237891</td>\n",
       "      <td>43686.311953</td>\n",
       "      <td>42091.993438</td>\n",
       "      <td>41291.485234</td>\n",
       "      <td>39872.907578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2791 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BTC-USD_High   BTC-USD_Low  AdrBalNtv0.01Cnt  AdrBalNtv0.1Cnt  \\\n",
       "2016-06-16    773.721985    696.523010         2707487.0        1194210.0   \n",
       "2016-06-17    775.356018    716.556030         2697438.0        1194425.0   \n",
       "2016-06-18    777.989990    733.929016         2700495.0        1195493.0   \n",
       "2016-06-19    766.620972    745.627991         2698863.0        1193758.0   \n",
       "2016-06-20    764.083984    732.726990         2707494.0        1197120.0   \n",
       "...                  ...           ...               ...              ...   \n",
       "2024-02-01  43243.167969  41879.191406        12619292.0        4572236.0   \n",
       "2024-02-02  43422.488281  42584.335938        12618955.0        4571607.0   \n",
       "2024-02-03  43359.941406  42890.808594        12623419.0        4572105.0   \n",
       "2024-02-04  43097.644531  42374.832031        12612131.0        4571390.0   \n",
       "2024-02-05  43494.250000  42264.816406        12605783.0        4571143.0   \n",
       "\n",
       "            AdrBalNtv1Cnt  AdrBalNtv10Cnt  BlkSizeMeanByte    CapRealUSD  \\\n",
       "2016-06-16       486235.0        138918.0     9.576345e+05  5.504034e+09   \n",
       "2016-06-17       485785.0        138762.0     9.202532e+05  5.528068e+09   \n",
       "2016-06-18       486147.0        138745.0     8.705890e+05  5.549665e+09   \n",
       "2016-06-19       486101.0        138807.0     7.119917e+05  5.566891e+09   \n",
       "2016-06-20       487506.0        138842.0     7.968285e+05  5.573669e+09   \n",
       "...                   ...             ...              ...           ...   \n",
       "2024-02-01      1019807.0        154559.0     1.706556e+06  4.479779e+11   \n",
       "2024-02-02      1019919.0        154576.0     1.700535e+06  4.482718e+11   \n",
       "2024-02-03      1019723.0        154548.0     1.696869e+06  4.483556e+11   \n",
       "2024-02-04      1019829.0        154504.0     1.576919e+06  4.484874e+11   \n",
       "2024-02-05      1019671.0        154513.0     1.415660e+06  4.487452e+11   \n",
       "\n",
       "            FeeByteMeanNtv    FlowInExNtv  ...  BTC-USD_High_SMA_5  \\\n",
       "2016-06-16    5.800000e-07  103461.872142  ...          715.075403   \n",
       "2016-06-17    5.700000e-07  101196.210866  ...          733.177808   \n",
       "2016-06-18    5.200000e-07   71224.258522  ...          745.575000   \n",
       "2016-06-19    4.900000e-07   44574.581631  ...          757.998389   \n",
       "2016-06-20    5.900000e-07   75586.050007  ...          771.554590   \n",
       "...                    ...            ...  ...                 ...   \n",
       "2024-02-01    2.166770e-07   28549.993968  ...        43380.512500   \n",
       "2024-02-02    2.066230e-07   26037.366846  ...        43505.575000   \n",
       "2024-02-03    7.011070e-07   14200.160578  ...        43516.389844   \n",
       "2024-02-04    3.948680e-07   14806.864074  ...        43368.129687   \n",
       "2024-02-05    2.925980e-07   23439.165199  ...        43323.498437   \n",
       "\n",
       "            BTC-USD_Low_SMA_5  BTC-USD_High_SMA_10  BTC-USD_Low_SMA_10  \\\n",
       "2016-06-16         660.682800           651.692102          616.700800   \n",
       "2016-06-17         682.586206           670.201807          631.605005   \n",
       "2016-06-18         696.474609           689.716907          647.684906   \n",
       "2016-06-19         713.039404           708.158704          665.152606   \n",
       "2016-06-20         725.072607           726.654401          681.092804   \n",
       "...                       ...                  ...                 ...   \n",
       "2024-02-01       42080.950000         42217.319922        40923.807813   \n",
       "2024-02-02       42258.435156         42546.833594        41330.051953   \n",
       "2024-02-03       42472.930469         42834.449219        41668.253125   \n",
       "2024-02-04       42405.622656         43118.765625        41951.169922   \n",
       "2024-02-05       42398.796875         43247.251953        42195.082422   \n",
       "\n",
       "            BTC-USD_High_SMA_20  BTC-USD_Low_SMA_20  BTC-USD_High_SMA_50  \\\n",
       "2016-06-16           605.640051          574.958147           514.898219   \n",
       "2016-06-17           617.734201          587.150998           521.414319   \n",
       "2016-06-18           628.935699          598.238498           527.866439   \n",
       "2016-06-19           640.049298          609.371747           534.087119   \n",
       "2016-06-20           650.922598          619.974997           540.319218   \n",
       "...                         ...                 ...                  ...   \n",
       "2024-02-01         42447.216016        41136.506836         43648.887969   \n",
       "2024-02-02         42456.607422        41142.516406         43649.520547   \n",
       "2024-02-03         42471.324609        41200.826172         43654.962891   \n",
       "2024-02-04         42460.220703        41234.296875         43663.616875   \n",
       "2024-02-05         42456.619531        41243.237891         43686.311953   \n",
       "\n",
       "            BTC-USD_Low_SMA_50  BTC-USD_High_SMA_100  BTC-USD_Low_SMA_100  \n",
       "2016-06-16          498.599160            471.263739           460.382540  \n",
       "2016-06-17          504.197281            474.856979           463.432041  \n",
       "2016-06-18          509.955521            478.461759           466.638821  \n",
       "2016-06-19          515.914141            481.888709           469.924971  \n",
       "2016-06-20          521.610141            485.311599           473.151301  \n",
       "...                        ...                   ...                  ...  \n",
       "2024-02-01        42018.851875          40943.784688         39519.390898  \n",
       "2024-02-02        42035.196797          41026.671992         39608.143164  \n",
       "2024-02-03        42059.153594          41111.942305         39699.428008  \n",
       "2024-02-04        42072.187969          41200.536641         39789.007461  \n",
       "2024-02-05        42091.993438          41291.485234         39872.907578  \n",
       "\n",
       "[2791 rows x 39 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "#df = df.pct_change()*100\n",
    "\n",
    "# The first row will be NaN because there's no previous data to subtract from the first entry\n",
    "# If you wish to remove the NaN values, you can drop the first row\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb725d-2cf7-4957-89dc-da102b357f4e",
   "metadata": {},
   "source": [
    "## High Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1898d3a-692d-4e65-b1b5-915b3923e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th percentile predictions: [43639.92]\n",
      "50th percentile (median) predictions: [43640.133]\n",
      "90th percentile predictions: [43640.36]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd  # Make sure to import pandas\n",
    "\n",
    "from gluonts.torch.model.predictor import PyTorchPredictor\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.transform.feature import MissingValueImputation\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'dudley_high.pth'\n",
    "trained_predictor = torch.load(model_path)\n",
    "\n",
    "# Assuming df is your DataFrame with the data\n",
    "target_column = 'BTC-USD_High'  # Replace with your target column name\n",
    "\n",
    "# Ensure the DataFrame's index is a datetime index and set the frequency explicitly if needed\n",
    "df.index = pd.to_datetime(df.index)\n",
    "freq = \"D\"  # Set the frequency of your data, e.g., 'D' for daily. Adjust as needed.\n",
    "df = df.asfreq(freq)\n",
    "\n",
    "# Define the prediction length\n",
    "prediction_length = 1  # Set your prediction length\n",
    "context_length = 7  # Set your prediction length\n",
    "\n",
    "# Select dynamic features from the DataFrame, excluding the target column\n",
    "past_dynamic_feature_columns = df.columns.drop(target_column)  # This excludes the target column\n",
    "\n",
    "# Extract dynamic features as a numpy array\n",
    "past_dynamic_features = df[past_dynamic_feature_columns].values.transpose()\n",
    "\n",
    "# Assuming all dynamic features are known in the future, adjust the dimensions accordingly\n",
    "past_dynamic_dims = [1] * len(past_dynamic_feature_columns)  # Adjust based on actual dynamic features\n",
    "\n",
    "# Adjust the slicing for dynamic features for the test dataset to ensure correct dimensions\n",
    "inference_past_dynamic_features_sliced = past_dynamic_features[:, -context_length:]\n",
    "\n",
    "# Correct forecast start date to the day after the last day in the dataset\n",
    "forecast_start_date = df.index[-1] + pd.Timedelta(days=2)\n",
    "\n",
    "# Setup for inference remains the same, assuming the last context_length days are used for input\n",
    "inference_data = ListDataset([\n",
    "    {\n",
    "        \"start\": forecast_start_date,\n",
    "        \"target\": df[target_column][-context_length:].values,\n",
    "        \"past_feat_dynamic_real\": inference_past_dynamic_features_sliced\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Initialize lists to store predictions for the 50th, 10th, and 90th percentiles\n",
    "high_p50_predictions = []\n",
    "high_p10_predictions = []\n",
    "high_p90_predictions = []\n",
    "\n",
    "# Perform inference and capture the desired quantiles\n",
    "for forecast in trained_predictor.predict(inference_data):\n",
    "    high_p50_predictions.append(forecast.quantile(0.5))  # Median\n",
    "    high_p10_predictions.append(forecast.quantile(0.1))  # 10th Percentile\n",
    "    high_p90_predictions.append(forecast.quantile(0.9))  # 90th Percentile\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "high_p50_predictions = np.array(high_p50_predictions).flatten()[:prediction_length]\n",
    "high_p10_predictions = np.array(high_p10_predictions).flatten()[:prediction_length]\n",
    "high_p90_predictions = np.array(high_p90_predictions).flatten()[:prediction_length]\n",
    "\n",
    "# Output the sizes to confirm they match prediction_length and print the percentile predictions\n",
    "print(f\"10th percentile predictions: {high_p10_predictions}\")\n",
    "print(f\"50th percentile (median) predictions: {high_p50_predictions}\")\n",
    "print(f\"90th percentile predictions: {high_p90_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d79fca-1377-43f6-a3e1-858f2013609f",
   "metadata": {},
   "source": [
    "## Low Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f54d1e8-27f3-4753-8f7f-69124756b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th percentile predictions: [41860.27]\n",
      "50th percentile (median) predictions: [41864.945]\n",
      "90th percentile predictions: [41868.94]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd  # Make sure to import pandas\n",
    "\n",
    "from gluonts.torch.model.predictor import PyTorchPredictor\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.transform.feature import MissingValueImputation\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'dudley_low.pth'\n",
    "trained_predictor = torch.load(model_path)\n",
    "\n",
    "# Assuming df is your DataFrame with the data\n",
    "target_column = 'BTC-USD_Low'  # Replace with your target column name\n",
    "\n",
    "# Ensure the DataFrame's index is a datetime index and set the frequency explicitly if needed\n",
    "df.index = pd.to_datetime(df.index)\n",
    "freq = \"D\"  # Set the frequency of your data, e.g., 'D' for daily. Adjust as needed.\n",
    "df = df.asfreq(freq)\n",
    "\n",
    "# Define the prediction length\n",
    "prediction_length = 1  # Set your prediction length\n",
    "context_length = 7  # Set your prediction length\n",
    "\n",
    "# Select dynamic features from the DataFrame, excluding the target column\n",
    "past_dynamic_feature_columns = df.columns.drop(target_column)  # This excludes the target column\n",
    "\n",
    "# Extract dynamic features as a numpy array\n",
    "past_dynamic_features = df[past_dynamic_feature_columns].values.transpose()\n",
    "\n",
    "# Assuming all dynamic features are known in the future, adjust the dimensions accordingly\n",
    "past_dynamic_dims = [1] * len(past_dynamic_feature_columns)  # Adjust based on actual dynamic features\n",
    "\n",
    "# Adjust the slicing for dynamic features for the test dataset to ensure correct dimensions\n",
    "inference_past_dynamic_features_sliced = past_dynamic_features[:, -context_length:]\n",
    "\n",
    "# Correct forecast start date to the day after the last day in the dataset\n",
    "forecast_start_date = df.index[-1] + pd.Timedelta(days=2)\n",
    "\n",
    "# Setup for inference remains the same, assuming the last context_length days are used for input\n",
    "inference_data = ListDataset([\n",
    "    {\n",
    "        \"start\": forecast_start_date,\n",
    "        \"target\": df[target_column][-context_length:].values,\n",
    "        \"past_feat_dynamic_real\": inference_past_dynamic_features_sliced\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Initialize lists to store predictions for the 50th, 10th, and 90th percentiles\n",
    "low_p50_predictions = []\n",
    "low_p10_predictions = []\n",
    "low_p90_predictions = []\n",
    "\n",
    "# Perform inference and capture the desired quantiles\n",
    "for forecast in trained_predictor.predict(inference_data):\n",
    "    low_p50_predictions.append(forecast.quantile(0.5))  # Median\n",
    "    low_p10_predictions.append(forecast.quantile(0.1))  # 10th Percentile\n",
    "    low_p90_predictions.append(forecast.quantile(0.9))  # 90th Percentile\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "low_p50_predictions = np.array(low_p50_predictions).flatten()[:prediction_length]\n",
    "low_p10_predictions = np.array(low_p10_predictions).flatten()[:prediction_length]\n",
    "low_p90_predictions = np.array(low_p90_predictions).flatten()[:prediction_length]\n",
    "\n",
    "# Output the sizes to confirm they match prediction_length and print the percentile predictions\n",
    "print(f\"10th percentile predictions: {low_p10_predictions}\")\n",
    "print(f\"50th percentile (median) predictions: {low_p50_predictions}\")\n",
    "print(f\"90th percentile predictions: {low_p90_predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41690316-4642-46d5-b4e8-cfc32ad15b47",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8834f3-5c64-4d8a-8f3e-5cc2edb56479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>8.40</td>\n",
       "      <td>8.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-06</th>\n",
       "      <td>8.38</td>\n",
       "      <td>8.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            High   Low\n",
       "Date                  \n",
       "2024-02-05  8.40  8.21\n",
       "2024-02-06  8.38  8.31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Adjusting the start and end times\n",
    "frequency = \"1d\"\n",
    "start_time = (datetime.now(pytz.timezone('US/Pacific')) - timedelta(days=1)).strftime('%Y-%m-%d')  # 5 days ago from the current date\n",
    "end_time = (datetime.now(pytz.timezone('US/Pacific')) + timedelta(days=2)).strftime('%Y-%m-%d')  # Current date in US/Pacific\n",
    "\n",
    "# Retrieve historical data and store it in a dictionary\n",
    "btcc_df = yf.download(\"BTCC-B.TO\", start=start_time, end=end_time, interval=frequency)[['High', 'Low']]\n",
    "btcc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043b0109-ee8b-4cbc-a8fe-50c108d45d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Low_P10_Prediction</th>\n",
       "      <th>Low_P50_Prediction</th>\n",
       "      <th>Low_P90_Prediction</th>\n",
       "      <th>High_P10_Prediction</th>\n",
       "      <th>High_P50_Prediction</th>\n",
       "      <th>High_P90_Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-07</th>\n",
       "      <td>8.105291</td>\n",
       "      <td>8.106196</td>\n",
       "      <td>8.10697</td>\n",
       "      <td>8.449881</td>\n",
       "      <td>8.449922</td>\n",
       "      <td>8.449965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Low_P10_Prediction  Low_P50_Prediction  Low_P90_Prediction  \\\n",
       "Date                                                                     \n",
       "2024-02-07            8.105291            8.106196             8.10697   \n",
       "\n",
       "            High_P10_Prediction  High_P50_Prediction  High_P90_Prediction  \n",
       "Date                                                                       \n",
       "2024-02-07             8.449881             8.449922             8.449965  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the variables for 10th and 90th percentile predictions are defined and populated\n",
    "# Ensure all predictions are properly shaped\n",
    "low_p10_predictions = np.array(low_p10_predictions).flatten()\n",
    "low_p90_predictions = np.array(low_p90_predictions).flatten()\n",
    "high_p10_predictions = np.array(high_p10_predictions).flatten()\n",
    "high_p90_predictions = np.array(high_p90_predictions).flatten()\n",
    "\n",
    "# Generate forecast dates starting from the day after the last day in the dataset\n",
    "forecast_dates = pd.date_range(start=forecast_start_date, periods=prediction_length, freq=freq)\n",
    "\n",
    "# Check if the length of predictions matches the expected prediction_length for all\n",
    "assert len(low_p50_predictions) == prediction_length, \"Mismatch in low_p50_predictions length\"\n",
    "assert len(high_p50_predictions) == prediction_length, \"Mismatch in high_p50_predictions length\"\n",
    "assert len(low_p10_predictions) == prediction_length, \"Mismatch in low_p10_predictions length\"\n",
    "assert len(low_p90_predictions) == prediction_length, \"Mismatch in low_p90_predictions length\"\n",
    "assert len(high_p10_predictions) == prediction_length, \"Mismatch in high_p10_predictions length\"\n",
    "assert len(high_p90_predictions) == prediction_length, \"Mismatch in high_p90_predictions length\"\n",
    "\n",
    "# Create a DataFrame with forecast dates and predictions\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Date': forecast_dates,\n",
    "    'Low_P10_Prediction': low_p10_predictions,\n",
    "    'Low_P50_Prediction': low_p50_predictions,\n",
    "    'Low_P90_Prediction': low_p90_predictions,\n",
    "    'High_P10_Prediction': high_p10_predictions,\n",
    "    'High_P50_Prediction': high_p50_predictions,\n",
    "    'High_P90_Prediction': high_p90_predictions\n",
    "})\n",
    "\n",
    "# Optionally, set the date as the index of the DataFrame\n",
    "forecast_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "(forecast_df * 1.3514) / 6979.3879\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebf67b12-0b8c-4a6b-b3b6-6067c427841a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming btcc_df and forecast_df are already defined\n",
    "last_high = btcc_df['High'].iloc[-1]\n",
    "last_low = btcc_df['Low'].iloc[-1]\n",
    "\n",
    "future_highs = []\n",
    "future_lows = []\n",
    "\n",
    "for _, row in forecast_df.iterrows():\n",
    "    new_high = last_high * (1 + row['High_P50_Prediction'] / 100)\n",
    "    new_low = last_low * (1 + row['Low_P50_Prediction'] / 100)\n",
    "\n",
    "    # Ensure new_low is not greater than new_high\n",
    "    if new_low > new_high:\n",
    "        new_low = new_high  # Adjust new_low to match new_high if it's greater\n",
    "\n",
    "    future_highs.append(new_high)\n",
    "    future_lows.append(new_low)\n",
    "    last_high, last_low = new_high, new_low  # Update last known values for the next iteration\n",
    "\n",
    "future_df = pd.DataFrame({\n",
    "    'Low': future_lows,\n",
    "    'High': future_highs\n",
    "}, index=forecast_df.index)\n",
    "\n",
    "# Reset btcc_df index to use 'Date' if not already, and concatenate with future_df\n",
    "btcc_df = btcc_df.reset_index().set_index('Date')\n",
    "final_df = pd.concat([btcc_df, future_df])\n",
    "\n",
    "# Sort final_df by the index (Date) to ensure proper order\n",
    "final_df.sort_index(inplace=True)\n",
    "\n",
    "# Explicitly set the column order for final_df to ensure 'Low' comes before 'High'\n",
    "final_df = final_df[['Low', 'High'] + [col for col in final_df.columns if col not in ['Low', 'High']]]\n",
    "\n",
    "# Add the percentage difference column to final_df\n",
    "final_df['Percentage Difference'] = ((final_df['High'] - final_df['Low']) / final_df['Low']) * 100"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdf853b5-ba78-4239-bbe6-337fa4c8d167",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the rest of your code is defined above and final_df is already created\n",
    "\n",
    "# Get today's date using pd.Timestamp and define the end date as today + 5 days\n",
    "today = pd.Timestamp('today').normalize()  # normalize to remove time component\n",
    "end_date = today + pd.Timedelta(days=5)\n",
    "\n",
    "# Filter final_df for rows where the index (Date) is between today and the next 5 days\n",
    "filtered_df = final_df.loc[today:end_date]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
