{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b0bfb3-96d6-48cc-b261-6ed2924aad11",
   "metadata": {},
   "source": [
    "# Petunia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98425636-8a8b-421f-afd8-68dccc65159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591158d4-01f9-494c-80e0-ab514681f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"amalgamated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bf6ca-f958-40e1-a27a-34292276ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "vars = [\n",
    "    \"Wasserstein_Distance\",\n",
    "    \"AdrBalNtv0.01Cnt\",\n",
    "    \"AdrBalNtv0.1Cnt\",\n",
    "    \"AdrBalNtv1Cnt\",\n",
    "    \"AdrBalNtv10Cnt\",\n",
    "    \"BlkSizeMeanByte\",\n",
    "    \"CapRealUSD\",\n",
    "    \"FeeByteMeanNtv\",\n",
    "    \"FlowInExNtv\",\n",
    "    \"FlowOutExNtv\",\n",
    "    \"FlowTfrFromExCnt\",\n",
    "    \"HashRate\",\n",
    "    \"NDF\",\n",
    "    \"SplyAct1d\",\n",
    "    \"SplyActPct1yr\",\n",
    "    \"TxCnt\",\n",
    "    \"VelCur1yr\"\n",
    "]\n",
    "\n",
    "df = df[vars]\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "df.dropna(inplace=True)\n",
    "df[vars] = scaler.fit_transform(df[vars])\n",
    "\n",
    "# Convert all specified variables to float64\n",
    "for var in vars:\n",
    "    if var in df.columns:  # Check if the variable is in the DataFrame\n",
    "        df[var] = df[var].astype('float64')\n",
    "        \n",
    "# Assert no NaNs in the DataFrame\n",
    "assert not df.isna().any().any(), \"DataFrame contains NaN values\"\n",
    "\n",
    "# Assert all values are finite in the DataFrame\n",
    "assert np.isfinite(df.values).all(), \"DataFrame contains infinite values\"\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ae61c-85c4-4789-93c6-e473fb853a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.transform.feature import MissingValueImputation\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "                                   \n",
    "# Assuming df is your DataFrame with the data\n",
    "target_column = 'Wasserstein_Distance'  # Replace with your target column name\n",
    "\n",
    "# Ensure the DataFrame's index is a datetime index and set the frequency explicitly if needed\n",
    "df.index = pd.to_datetime(df.index)\n",
    "freq = \"D\"  # Set the frequency of your data, e.g., 'D' for daily. Adjust as needed.\n",
    "df = df.asfreq(freq)\n",
    "\n",
    "# Define the prediction length\n",
    "prediction_length = 1  # Set your prediction length\n",
    "\n",
    "# Select dynamic features from the DataFrame, excluding the target column\n",
    "past_dynamic_feature_columns = df.columns.drop(target_column)  # This excludes the target column\n",
    "\n",
    "# Extract dynamic features as a numpy array\n",
    "past_dynamic_features = df[past_dynamic_feature_columns].values.transpose()\n",
    "\n",
    "# Assuming all dynamic features are known in the future, adjust the dimensions accordingly\n",
    "past_dynamic_dims  = [1] * len(past_dynamic_feature_columns)  # Adjust based on actual dynamic features\n",
    "\n",
    "# Make sure the lengths match when creating ListDataset\n",
    "training_data = ListDataset([\n",
    "    {\n",
    "        \"start\": df.index[0],\n",
    "        \"target\": df[target_column][:-prediction_length],\n",
    "        \"past_feat_dynamic_real\": past_dynamic_features[:, :-prediction_length]\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Adjust the slicing for dynamic features for the test dataset to ensure correct dimensions\n",
    "test_past_dynamic_features_sliced = past_dynamic_features[:, -prediction_length:]\n",
    "\n",
    "test_data = ListDataset([\n",
    "    {\n",
    "        \"start\": df.index[-prediction_length],\n",
    "        \"target\": df[target_column][-prediction_length:].values,\n",
    "        \"past_feat_dynamic_real\": test_past_dynamic_features_sliced\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Improved initialization of the Temporal Fusion Transformer Estimator\n",
    "estimator = TemporalFusionTransformerEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=7,  # Adjust based on your specific needs\n",
    "    num_heads=4,\n",
    "    hidden_dim=1024,\n",
    "    variable_dim=1024,\n",
    "    past_dynamic_dims=past_dynamic_dims,\n",
    "    quantiles=[0.1, 0.5, 0.9],  # Specifying quantiles for forecasting\n",
    "    lr=1e-4,  # Adjusted learning rate\n",
    "    weight_decay=1e-08,\n",
    "    dropout_rate=0.15,\n",
    "    patience=10,\n",
    "    batch_size=32,\n",
    "    num_batches_per_epoch=50,\n",
    "    trainer_kwargs={'max_epochs': 5000, 'gradient_clip_val': 1.0},  # Example to include gradient clipping\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "predictor = estimator.train(training_data)\n",
    "\n",
    "# Debugging: Print dimensions to verify alignment\n",
    "print(f\"Past Dynamic Features Training Shape: {past_dynamic_features[:, :-prediction_length].shape}\")\n",
    "print(f\"Past Dynamic Features Test Shape: {test_past_dynamic_features_sliced.shape}\")\n",
    "\n",
    "# Collect actual and predicted values for evaluation, including percentiles\n",
    "actuals = df[target_column][-prediction_length:].values\n",
    "mean_predictions = []\n",
    "p10_predictions = []\n",
    "p50_predictions = []\n",
    "p90_predictions = []\n",
    "\n",
    "for forecast in predictor.predict(test_data):\n",
    "    mean_predictions.append(forecast.mean)\n",
    "    p10_predictions.append(forecast.quantile(0.1))\n",
    "    p50_predictions.append(forecast.quantile(0.5))  # Median\n",
    "    p90_predictions.append(forecast.quantile(0.9))\n",
    "\n",
    "# Convert lists to numpy arrays for slicing\n",
    "mean_predictions = np.array(mean_predictions).flatten()[:prediction_length]\n",
    "p10_predictions = np.array(p10_predictions).flatten()[:prediction_length]\n",
    "p50_predictions = np.array(p50_predictions).flatten()[:prediction_length]\n",
    "p90_predictions = np.array(p90_predictions).flatten()[:prediction_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960df258-1313-46e2-92ba-0d8b39afa15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row = df.iloc[-1:, :]\n",
    "last_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cc2ba-d3a8-4bfe-9945-bc79f261cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p50_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fd538-c47b-4b5a-9131-a6f5995b01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sMAPE\n",
    "def calculate_smape(forecasts, actuals):\n",
    "    return 100 * np.mean(2 * np.abs(forecasts - actuals) / (np.abs(actuals) + np.abs(forecasts)))\n",
    "\n",
    "# Calculate standard evaluation metrics for mean predictions\n",
    "mae = np.mean(np.abs(mean_predictions - actuals))\n",
    "rmse = np.sqrt(np.mean(np.square(mean_predictions - actuals)))\n",
    "mape = np.mean(np.abs((mean_predictions - actuals) / actuals)) * 100\n",
    "smape = calculate_smape(mean_predictions, actuals)\n",
    "\n",
    "# Calculate the percentage of actuals within the 10th to 90th percentile range\n",
    "within_range = np.sum((actuals >= p10_predictions) & (actuals <= p90_predictions)) / len(actuals) * 100\n",
    "\n",
    "print(\"Evaluation Metrics for Mean Predictions:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"sMAPE: {smape:.2f}%\")\n",
    "print(f\"Percentage of Actuals within P10-P90 Interval: {within_range:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa71034-caf4-433e-ae72-05fa5395ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define a path to save the model\n",
    "model_save_path = 'dudley_high.pth'\n",
    "\n",
    "# Assuming 'predictor' is the trained model from DeepAREstimator\n",
    "torch.save(predictor, model_save_path)\n",
    "\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec76915-33c0-402f-96dc-ed8e1641c23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
