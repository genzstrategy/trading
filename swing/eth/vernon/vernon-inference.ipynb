{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b0bfb3-96d6-48cc-b261-6ed2924aad11",
   "metadata": {},
   "source": [
    "# Vernon Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98425636-8a8b-421f-afd8-68dccc65159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9da0bc-1413-4bcd-8b51-9286d86fdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"amalgamated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c9cd48-db18-40fa-bbdc-d20473815d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = [\n",
    "    \"ETH-CAD_High\",\n",
    "    \"ETH-CAD_Low\",\n",
    "    \"BTC-CAD_High\",\n",
    "    \"BTC-CAD_Low\",\n",
    "    \"AdrBalNtv0.01Cnt\",\n",
    "    \"AdrBalNtv0.1Cnt\",\n",
    "    \"AdrBalNtv1Cnt\",\n",
    "    \"AdrBalNtv10Cnt\",\n",
    "    \"BlkSizeMeanByte\",\n",
    "    \"CapRealUSD\",\n",
    "    \"FeeByteMeanNtv\",\n",
    "    \"FlowInExNtv\",\n",
    "    \"FlowOutExNtv\",\n",
    "    \"FlowTfrFromExCnt\",\n",
    "    \"GasUsedTxMean\",\n",
    "    \"NDF\",\n",
    "    \"SplyAct1d\",\n",
    "    \"SplyActPct1yr\",\n",
    "    \"TxCnt\",\n",
    "    \"VelCur1yr\",\n",
    "    'Global_Liquidity_Index'\n",
    "]\n",
    "\n",
    "\n",
    "df = df[vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5001d85-6388-4d46-b27b-36b8d6ae11d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ETH-CAD_High</th>\n",
       "      <th>ETH-CAD_Low</th>\n",
       "      <th>BTC-CAD_High</th>\n",
       "      <th>BTC-CAD_Low</th>\n",
       "      <th>AdrBalNtv0.01Cnt</th>\n",
       "      <th>AdrBalNtv0.1Cnt</th>\n",
       "      <th>AdrBalNtv1Cnt</th>\n",
       "      <th>AdrBalNtv10Cnt</th>\n",
       "      <th>BlkSizeMeanByte</th>\n",
       "      <th>CapRealUSD</th>\n",
       "      <th>...</th>\n",
       "      <th>FlowInExNtv</th>\n",
       "      <th>FlowOutExNtv</th>\n",
       "      <th>FlowTfrFromExCnt</th>\n",
       "      <th>GasUsedTxMean</th>\n",
       "      <th>NDF</th>\n",
       "      <th>SplyAct1d</th>\n",
       "      <th>SplyActPct1yr</th>\n",
       "      <th>TxCnt</th>\n",
       "      <th>VelCur1yr</th>\n",
       "      <th>Global_Liquidity_Index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>192.649857</td>\n",
       "      <td>180.575409</td>\n",
       "      <td>5246.870117</td>\n",
       "      <td>5057.234375</td>\n",
       "      <td>5725623</td>\n",
       "      <td>2553149</td>\n",
       "      <td>948660</td>\n",
       "      <td>240984</td>\n",
       "      <td>13665.922841</td>\n",
       "      <td>2.887769e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>392978.197625</td>\n",
       "      <td>4.197261e+05</td>\n",
       "      <td>15602</td>\n",
       "      <td>73640.350123</td>\n",
       "      <td>0.635803</td>\n",
       "      <td>1.122109e+07</td>\n",
       "      <td>72.295246</td>\n",
       "      <td>448168</td>\n",
       "      <td>11.758691</td>\n",
       "      <td>10677.329758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>213.886246</td>\n",
       "      <td>191.795868</td>\n",
       "      <td>5388.129883</td>\n",
       "      <td>5191.100586</td>\n",
       "      <td>5729156</td>\n",
       "      <td>2554121</td>\n",
       "      <td>948649</td>\n",
       "      <td>241344</td>\n",
       "      <td>18126.132355</td>\n",
       "      <td>2.899081e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>790231.396805</td>\n",
       "      <td>7.295684e+05</td>\n",
       "      <td>21191</td>\n",
       "      <td>60064.995705</td>\n",
       "      <td>0.636376</td>\n",
       "      <td>1.034321e+07</td>\n",
       "      <td>72.264375</td>\n",
       "      <td>589959</td>\n",
       "      <td>11.643999</td>\n",
       "      <td>11979.420951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>212.573837</td>\n",
       "      <td>198.451218</td>\n",
       "      <td>5362.917969</td>\n",
       "      <td>5159.898926</td>\n",
       "      <td>5733108</td>\n",
       "      <td>2555952</td>\n",
       "      <td>948798</td>\n",
       "      <td>240985</td>\n",
       "      <td>19342.790323</td>\n",
       "      <td>2.898172e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>654169.139428</td>\n",
       "      <td>6.504445e+05</td>\n",
       "      <td>20336</td>\n",
       "      <td>56967.882419</td>\n",
       "      <td>0.636168</td>\n",
       "      <td>9.498194e+06</td>\n",
       "      <td>72.221087</td>\n",
       "      <td>596620</td>\n",
       "      <td>11.529535</td>\n",
       "      <td>11920.554140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>211.151260</td>\n",
       "      <td>199.472687</td>\n",
       "      <td>5172.040527</td>\n",
       "      <td>5087.920898</td>\n",
       "      <td>5732036</td>\n",
       "      <td>2553742</td>\n",
       "      <td>948728</td>\n",
       "      <td>240912</td>\n",
       "      <td>18872.124595</td>\n",
       "      <td>2.909478e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>680241.364599</td>\n",
       "      <td>1.184203e+06</td>\n",
       "      <td>21142</td>\n",
       "      <td>61527.769639</td>\n",
       "      <td>0.636041</td>\n",
       "      <td>1.421223e+07</td>\n",
       "      <td>72.193628</td>\n",
       "      <td>549398</td>\n",
       "      <td>11.365511</td>\n",
       "      <td>11671.058247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05</th>\n",
       "      <td>215.118729</td>\n",
       "      <td>206.441116</td>\n",
       "      <td>5223.182617</td>\n",
       "      <td>5131.535645</td>\n",
       "      <td>5730961</td>\n",
       "      <td>2549563</td>\n",
       "      <td>944626</td>\n",
       "      <td>240101</td>\n",
       "      <td>17351.082028</td>\n",
       "      <td>2.904683e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>631193.304059</td>\n",
       "      <td>1.490288e+06</td>\n",
       "      <td>17419</td>\n",
       "      <td>63305.365839</td>\n",
       "      <td>0.637131</td>\n",
       "      <td>1.288664e+07</td>\n",
       "      <td>72.080216</td>\n",
       "      <td>511189</td>\n",
       "      <td>11.221790</td>\n",
       "      <td>10776.278107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-02</th>\n",
       "      <td>3088.287109</td>\n",
       "      <td>3013.004883</td>\n",
       "      <td>57877.652344</td>\n",
       "      <td>56241.656250</td>\n",
       "      <td>24732407</td>\n",
       "      <td>5195599</td>\n",
       "      <td>1729374</td>\n",
       "      <td>344511</td>\n",
       "      <td>149793.691422</td>\n",
       "      <td>1.667400e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>224978.990905</td>\n",
       "      <td>3.353659e+05</td>\n",
       "      <td>103742</td>\n",
       "      <td>97339.905076</td>\n",
       "      <td>0.677616</td>\n",
       "      <td>1.248846e+07</td>\n",
       "      <td>35.971409</td>\n",
       "      <td>1108172</td>\n",
       "      <td>6.045875</td>\n",
       "      <td>122242.497035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-03</th>\n",
       "      <td>3107.525391</td>\n",
       "      <td>3067.862793</td>\n",
       "      <td>58432.644531</td>\n",
       "      <td>57157.976562</td>\n",
       "      <td>24733683</td>\n",
       "      <td>5185916</td>\n",
       "      <td>1728662</td>\n",
       "      <td>344395</td>\n",
       "      <td>153626.986371</td>\n",
       "      <td>1.665798e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>88094.093613</td>\n",
       "      <td>1.106255e+05</td>\n",
       "      <td>93780</td>\n",
       "      <td>102296.542796</td>\n",
       "      <td>0.677607</td>\n",
       "      <td>1.176235e+07</td>\n",
       "      <td>35.937981</td>\n",
       "      <td>1054604</td>\n",
       "      <td>6.036129</td>\n",
       "      <td>121766.009277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-04</th>\n",
       "      <td>3136.216309</td>\n",
       "      <td>3090.657471</td>\n",
       "      <td>58429.691406</td>\n",
       "      <td>57797.507812</td>\n",
       "      <td>24723204</td>\n",
       "      <td>5169224</td>\n",
       "      <td>1728661</td>\n",
       "      <td>344281</td>\n",
       "      <td>153334.124877</td>\n",
       "      <td>1.664679e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>155456.643029</td>\n",
       "      <td>1.523433e+05</td>\n",
       "      <td>89438</td>\n",
       "      <td>102710.415043</td>\n",
       "      <td>0.678024</td>\n",
       "      <td>9.964788e+06</td>\n",
       "      <td>35.919804</td>\n",
       "      <td>1049649</td>\n",
       "      <td>6.033078</td>\n",
       "      <td>122454.072998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>3111.508301</td>\n",
       "      <td>3059.306396</td>\n",
       "      <td>58076.230469</td>\n",
       "      <td>57048.390625</td>\n",
       "      <td>24713162</td>\n",
       "      <td>5146142</td>\n",
       "      <td>1728854</td>\n",
       "      <td>344070</td>\n",
       "      <td>161689.073003</td>\n",
       "      <td>1.664033e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>230617.498979</td>\n",
       "      <td>3.692275e+05</td>\n",
       "      <td>95967</td>\n",
       "      <td>96329.680096</td>\n",
       "      <td>0.677587</td>\n",
       "      <td>1.135770e+07</td>\n",
       "      <td>35.852078</td>\n",
       "      <td>1120237</td>\n",
       "      <td>6.044788</td>\n",
       "      <td>123316.901832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-06</th>\n",
       "      <td>3153.139648</td>\n",
       "      <td>3061.027832</td>\n",
       "      <td>58741.941406</td>\n",
       "      <td>56991.148438</td>\n",
       "      <td>24713335</td>\n",
       "      <td>5149706</td>\n",
       "      <td>1729422</td>\n",
       "      <td>344134</td>\n",
       "      <td>139481.761263</td>\n",
       "      <td>1.672634e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>347428.606404</td>\n",
       "      <td>3.143163e+05</td>\n",
       "      <td>103803</td>\n",
       "      <td>99786.480486</td>\n",
       "      <td>0.677327</td>\n",
       "      <td>1.191870e+07</td>\n",
       "      <td>35.867592</td>\n",
       "      <td>1080846</td>\n",
       "      <td>6.048036</td>\n",
       "      <td>123971.729975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1863 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ETH-CAD_High  ETH-CAD_Low  BTC-CAD_High   BTC-CAD_Low  \\\n",
       "time                                                                \n",
       "2019-01-01    192.649857   180.575409   5246.870117   5057.234375   \n",
       "2019-01-02    213.886246   191.795868   5388.129883   5191.100586   \n",
       "2019-01-03    212.573837   198.451218   5362.917969   5159.898926   \n",
       "2019-01-04    211.151260   199.472687   5172.040527   5087.920898   \n",
       "2019-01-05    215.118729   206.441116   5223.182617   5131.535645   \n",
       "...                  ...          ...           ...           ...   \n",
       "2024-02-02   3088.287109  3013.004883  57877.652344  56241.656250   \n",
       "2024-02-03   3107.525391  3067.862793  58432.644531  57157.976562   \n",
       "2024-02-04   3136.216309  3090.657471  58429.691406  57797.507812   \n",
       "2024-02-05   3111.508301  3059.306396  58076.230469  57048.390625   \n",
       "2024-02-06   3153.139648  3061.027832  58741.941406  56991.148438   \n",
       "\n",
       "            AdrBalNtv0.01Cnt  AdrBalNtv0.1Cnt  AdrBalNtv1Cnt  AdrBalNtv10Cnt  \\\n",
       "time                                                                           \n",
       "2019-01-01           5725623          2553149         948660          240984   \n",
       "2019-01-02           5729156          2554121         948649          241344   \n",
       "2019-01-03           5733108          2555952         948798          240985   \n",
       "2019-01-04           5732036          2553742         948728          240912   \n",
       "2019-01-05           5730961          2549563         944626          240101   \n",
       "...                      ...              ...            ...             ...   \n",
       "2024-02-02          24732407          5195599        1729374          344511   \n",
       "2024-02-03          24733683          5185916        1728662          344395   \n",
       "2024-02-04          24723204          5169224        1728661          344281   \n",
       "2024-02-05          24713162          5146142        1728854          344070   \n",
       "2024-02-06          24713335          5149706        1729422          344134   \n",
       "\n",
       "            BlkSizeMeanByte    CapRealUSD  ...    FlowInExNtv  FlowOutExNtv  \\\n",
       "time                                       ...                                \n",
       "2019-01-01     13665.922841  2.887769e+10  ...  392978.197625  4.197261e+05   \n",
       "2019-01-02     18126.132355  2.899081e+10  ...  790231.396805  7.295684e+05   \n",
       "2019-01-03     19342.790323  2.898172e+10  ...  654169.139428  6.504445e+05   \n",
       "2019-01-04     18872.124595  2.909478e+10  ...  680241.364599  1.184203e+06   \n",
       "2019-01-05     17351.082028  2.904683e+10  ...  631193.304059  1.490288e+06   \n",
       "...                     ...           ...  ...            ...           ...   \n",
       "2024-02-02    149793.691422  1.667400e+11  ...  224978.990905  3.353659e+05   \n",
       "2024-02-03    153626.986371  1.665798e+11  ...   88094.093613  1.106255e+05   \n",
       "2024-02-04    153334.124877  1.664679e+11  ...  155456.643029  1.523433e+05   \n",
       "2024-02-05    161689.073003  1.664033e+11  ...  230617.498979  3.692275e+05   \n",
       "2024-02-06    139481.761263  1.672634e+11  ...  347428.606404  3.143163e+05   \n",
       "\n",
       "            FlowTfrFromExCnt  GasUsedTxMean       NDF     SplyAct1d  \\\n",
       "time                                                                  \n",
       "2019-01-01             15602   73640.350123  0.635803  1.122109e+07   \n",
       "2019-01-02             21191   60064.995705  0.636376  1.034321e+07   \n",
       "2019-01-03             20336   56967.882419  0.636168  9.498194e+06   \n",
       "2019-01-04             21142   61527.769639  0.636041  1.421223e+07   \n",
       "2019-01-05             17419   63305.365839  0.637131  1.288664e+07   \n",
       "...                      ...            ...       ...           ...   \n",
       "2024-02-02            103742   97339.905076  0.677616  1.248846e+07   \n",
       "2024-02-03             93780  102296.542796  0.677607  1.176235e+07   \n",
       "2024-02-04             89438  102710.415043  0.678024  9.964788e+06   \n",
       "2024-02-05             95967   96329.680096  0.677587  1.135770e+07   \n",
       "2024-02-06            103803   99786.480486  0.677327  1.191870e+07   \n",
       "\n",
       "            SplyActPct1yr    TxCnt  VelCur1yr  Global_Liquidity_Index  \n",
       "time                                                                   \n",
       "2019-01-01      72.295246   448168  11.758691            10677.329758  \n",
       "2019-01-02      72.264375   589959  11.643999            11979.420951  \n",
       "2019-01-03      72.221087   596620  11.529535            11920.554140  \n",
       "2019-01-04      72.193628   549398  11.365511            11671.058247  \n",
       "2019-01-05      72.080216   511189  11.221790            10776.278107  \n",
       "...                   ...      ...        ...                     ...  \n",
       "2024-02-02      35.971409  1108172   6.045875           122242.497035  \n",
       "2024-02-03      35.937981  1054604   6.036129           121766.009277  \n",
       "2024-02-04      35.919804  1049649   6.033078           122454.072998  \n",
       "2024-02-05      35.852078  1120237   6.044788           123316.901832  \n",
       "2024-02-06      35.867592  1080846   6.048036           123971.729975  \n",
       "\n",
       "[1863 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "#df = df.pct_change()*100\n",
    "\n",
    "# The first row will be NaN because there's no previous data to subtract from the first entry\n",
    "# If you wish to remove the NaN values, you can drop the first row\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb725d-2cf7-4957-89dc-da102b357f4e",
   "metadata": {},
   "source": [
    "## High Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1898d3a-692d-4e65-b1b5-915b3923e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th percentile predictions: [3214.5051]\n",
      "50th percentile (median) predictions: [3214.5347]\n",
      "90th percentile predictions: [3214.5654]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "from gluonts.torch.model.predictor import PyTorchPredictor\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.transform.feature import MissingValueImputation\n",
    "\n",
    "# Assuming df is your DataFrame with the data\n",
    "target_column = 'ETH-CAD_High'  # Replace with your target column name\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'vernon_high.pth'\n",
    "trained_predictor = torch.load(model_path)\n",
    "\n",
    "# Ensure the DataFrame's index is a datetime index and set the frequency explicitly if needed\n",
    "df.index = pd.to_datetime(df.index)\n",
    "freq = \"D\"  # Adjust the frequency of your data as needed\n",
    "df = df.asfreq(freq)\n",
    "\n",
    "# Define the prediction length and context length\n",
    "prediction_length = 1\n",
    "context_length = 7\n",
    "\n",
    "# Select dynamic features from the DataFrame, excluding the target column\n",
    "past_dynamic_feature_columns = df.columns.drop(target_column)  # Excludes the target column\n",
    "past_dynamic_features = df[past_dynamic_feature_columns].values.transpose()\n",
    "past_dynamic_dims = [1] * len(past_dynamic_feature_columns)  # Adjust based on actual dynamic features\n",
    "\n",
    "# Adjust the slicing for dynamic features for the test dataset\n",
    "inference_past_dynamic_features_sliced = past_dynamic_features[:, -context_length:]\n",
    "\n",
    "# Correct forecast start date\n",
    "forecast_start_date = df.index[-1] + pd.Timedelta(days=1)\n",
    "\n",
    "# Setup for inference using the last context_length days as input\n",
    "inference_data = ListDataset([\n",
    "    {\n",
    "        \"start\": forecast_start_date,\n",
    "        \"target\": df[target_column][-context_length:].values,\n",
    "        \"past_feat_dynamic_real\": inference_past_dynamic_features_sliced\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Initialize lists to store predictions for the 50th, 10th, and 90th percentiles\n",
    "high_p50_predictions = []\n",
    "high_p10_predictions = []\n",
    "high_p90_predictions = []\n",
    "\n",
    "# Perform inference and capture the desired quantiles\n",
    "for forecast in trained_predictor.predict(inference_data):\n",
    "    high_p50_predictions.append(forecast.quantile(0.5))  # Median\n",
    "    high_p10_predictions.append(forecast.quantile(0.1))  # 10th Percentile\n",
    "    high_p90_predictions.append(forecast.quantile(0.9))  # 90th Percentile\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "high_p50_predictions = np.array(high_p50_predictions).flatten()[:prediction_length]\n",
    "high_p10_predictions = np.array(high_p10_predictions).flatten()[:prediction_length]\n",
    "high_p90_predictions = np.array(high_p90_predictions).flatten()[:prediction_length]\n",
    "\n",
    "# Output the sizes to confirm they match prediction_length and print the percentile predictions\n",
    "print(f\"10th percentile predictions: {high_p10_predictions}\")\n",
    "print(f\"50th percentile (median) predictions: {high_p50_predictions}\")\n",
    "print(f\"90th percentile predictions: {high_p90_predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d79fca-1377-43f6-a3e1-858f2013609f",
   "metadata": {},
   "source": [
    "## Low Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f54d1e8-27f3-4753-8f7f-69124756b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th percentile predictions: [3122.636]\n",
      "50th percentile (median) predictions: [3122.7139]\n",
      "90th percentile predictions: [3122.7961]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "from gluonts.torch.model.predictor import PyTorchPredictor\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.transform.feature import MissingValueImputation\n",
    "\n",
    "# Assuming df is your DataFrame with the data\n",
    "target_column = 'ETH-CAD_Low'  # Replace with your target column name\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'vernon_low.pth'\n",
    "trained_predictor = torch.load(model_path)\n",
    "\n",
    "# Ensure the DataFrame's index is a datetime index and set the frequency explicitly if needed\n",
    "df.index = pd.to_datetime(df.index)\n",
    "freq = \"D\"  # Adjust the frequency of your data as needed\n",
    "df = df.asfreq(freq)\n",
    "\n",
    "# Define the prediction length and context length\n",
    "prediction_length = 1\n",
    "context_length = 7\n",
    "\n",
    "# Select dynamic features from the DataFrame, excluding the target column\n",
    "past_dynamic_feature_columns = df.columns.drop(target_column)  # Excludes the target column\n",
    "past_dynamic_features = df[past_dynamic_feature_columns].values.transpose()\n",
    "past_dynamic_dims = [1] * len(past_dynamic_feature_columns)  # Adjust based on actual dynamic features\n",
    "\n",
    "# Adjust the slicing for dynamic features for the test dataset\n",
    "inference_past_dynamic_features_sliced = past_dynamic_features[:, -context_length:]\n",
    "\n",
    "# Correct forecast start date\n",
    "forecast_start_date = df.index[-1] + pd.Timedelta(days=1)\n",
    "\n",
    "# Setup for inference using the last context_length days as input\n",
    "inference_data = ListDataset([\n",
    "    {\n",
    "        \"start\": forecast_start_date,\n",
    "        \"target\": df[target_column][-context_length:].values,\n",
    "        \"past_feat_dynamic_real\": inference_past_dynamic_features_sliced\n",
    "    }\n",
    "], freq=freq)\n",
    "\n",
    "# Initialize lists to store predictions for the 50th, 10th, and 90th percentiles\n",
    "low_p50_predictions = []\n",
    "low_p10_predictions = []\n",
    "low_p90_predictions = []\n",
    "\n",
    "# Perform inference and capture the desired quantiles\n",
    "for forecast in trained_predictor.predict(inference_data):\n",
    "    low_p50_predictions.append(forecast.quantile(0.5))  # Median\n",
    "    low_p10_predictions.append(forecast.quantile(0.1))  # 10th Percentile\n",
    "    low_p90_predictions.append(forecast.quantile(0.9))  # 90th Percentile\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "low_p50_predictions = np.array(low_p50_predictions).flatten()[:prediction_length]\n",
    "low_p10_predictions = np.array(low_p10_predictions).flatten()[:prediction_length]\n",
    "low_p90_predictions = np.array(low_p90_predictions).flatten()[:prediction_length]\n",
    "\n",
    "# Output the sizes to confirm they match prediction_length and print the percentile predictions\n",
    "print(f\"10th percentile predictions: {low_p10_predictions}\")\n",
    "print(f\"50th percentile (median) predictions: {low_p50_predictions}\")\n",
    "print(f\"90th percentile predictions: {low_p90_predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41690316-4642-46d5-b4e8-cfc32ad15b47",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8834f3-5c64-4d8a-8f3e-5cc2edb56479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>9.43</td>\n",
       "      <td>9.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-06</th>\n",
       "      <td>9.70</td>\n",
       "      <td>9.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            High   Low\n",
       "Date                  \n",
       "2024-02-05  9.43  9.27\n",
       "2024-02-06  9.70  9.47"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Adjusting the start and end times\n",
    "frequency = \"1d\"\n",
    "start_time = (datetime.now(pytz.timezone('US/Pacific')) - timedelta(days=1)).strftime('%Y-%m-%d')  # 5 days ago from the current date\n",
    "end_time = (datetime.now(pytz.timezone('US/Pacific')) + timedelta(days=1)).strftime('%Y-%m-%d')  # Current date in US/Pacific\n",
    "\n",
    "# Retrieve historical data and store it in a dictionary\n",
    "ETHc_df = yf.download(\"ETHH.TO\", start=start_time, end=end_time, interval=frequency)[['High', 'Low']]\n",
    "ETHc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043b0109-ee8b-4cbc-a8fe-50c108d45d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Low_P10_Prediction</th>\n",
       "      <th>Low_P50_Prediction</th>\n",
       "      <th>Low_P90_Prediction</th>\n",
       "      <th>High_P10_Prediction</th>\n",
       "      <th>High_P50_Prediction</th>\n",
       "      <th>High_P90_Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-07</th>\n",
       "      <td>9.393375</td>\n",
       "      <td>9.39361</td>\n",
       "      <td>9.393857</td>\n",
       "      <td>9.669732</td>\n",
       "      <td>9.669821</td>\n",
       "      <td>9.669913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Low_P10_Prediction  Low_P50_Prediction  Low_P90_Prediction  \\\n",
       "Date                                                                     \n",
       "2024-02-07            9.393375             9.39361            9.393857   \n",
       "\n",
       "            High_P10_Prediction  High_P50_Prediction  High_P90_Prediction  \n",
       "Date                                                                       \n",
       "2024-02-07             9.669732             9.669821             9.669913  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the variables for 10th and 90th percentile predictions are defined and populated\n",
    "# Ensure all predictions are properly shaped\n",
    "low_p10_predictions = np.array(low_p10_predictions).flatten()\n",
    "low_p90_predictions = np.array(low_p90_predictions).flatten()\n",
    "high_p10_predictions = np.array(high_p10_predictions).flatten()\n",
    "high_p90_predictions = np.array(high_p90_predictions).flatten()\n",
    "\n",
    "# Generate forecast dates starting from the day after the last day in the dataset\n",
    "forecast_dates = pd.date_range(start=forecast_start_date, periods=prediction_length, freq=freq)\n",
    "\n",
    "# Check if the length of predictions matches the expected prediction_length for all\n",
    "assert len(low_p50_predictions) == prediction_length, \"Mismatch in low_p50_predictions length\"\n",
    "assert len(high_p50_predictions) == prediction_length, \"Mismatch in high_p50_predictions length\"\n",
    "assert len(low_p10_predictions) == prediction_length, \"Mismatch in low_p10_predictions length\"\n",
    "assert len(low_p90_predictions) == prediction_length, \"Mismatch in low_p90_predictions length\"\n",
    "assert len(high_p10_predictions) == prediction_length, \"Mismatch in high_p10_predictions length\"\n",
    "assert len(high_p90_predictions) == prediction_length, \"Mismatch in high_p90_predictions length\"\n",
    "\n",
    "# Create a DataFrame with forecast dates and predictions\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Date': forecast_dates,\n",
    "    'Low_P10_Prediction': low_p10_predictions,\n",
    "    'Low_P50_Prediction': low_p50_predictions,\n",
    "    'Low_P90_Prediction': low_p90_predictions,\n",
    "    'High_P10_Prediction': high_p10_predictions,\n",
    "    'High_P50_Prediction': high_p50_predictions,\n",
    "    'High_P90_Prediction': high_p90_predictions\n",
    "})\n",
    "\n",
    "# Optionally, set the date as the index of the DataFrame\n",
    "forecast_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "forecast_df / 332.4296  # Assuming the division by 332.4296 is specific to your use case\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebf67b12-0b8c-4a6b-b3b6-6067c427841a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming ETHc_df and forecast_df are already defined\n",
    "last_high = ETHc_df['High'].iloc[-1]\n",
    "last_low = ETHc_df['Low'].iloc[-1]\n",
    "\n",
    "future_highs = []\n",
    "future_lows = []\n",
    "\n",
    "for _, row in forecast_df.iterrows():\n",
    "    new_high = last_high * (1 + row['High_P50_Prediction'] / 100)\n",
    "    new_low = last_low * (1 + row['Low_P50_Prediction'] / 100)\n",
    "\n",
    "    # Ensure new_low is not greater than new_high\n",
    "    if new_low > new_high:\n",
    "        new_low = new_high  # Adjust new_low to match new_high if it's greater\n",
    "\n",
    "    future_highs.append(new_high)\n",
    "    future_lows.append(new_low)\n",
    "    last_high, last_low = new_high, new_low  # Update last known values for the next iteration\n",
    "\n",
    "future_df = pd.DataFrame({\n",
    "    'Low': future_lows,\n",
    "    'High': future_highs\n",
    "}, index=forecast_df.index)\n",
    "\n",
    "# Reset ETHc_df index to use 'Date' if not already, and concatenate with future_df\n",
    "ETHc_df = ETHc_df.reset_index().set_index('Date')\n",
    "final_df = pd.concat([ETHc_df, future_df])\n",
    "\n",
    "# Sort final_df by the index (Date) to ensure proper order\n",
    "final_df.sort_index(inplace=True)\n",
    "\n",
    "# Explicitly set the column order for final_df to ensure 'Low' comes before 'High'\n",
    "final_df = final_df[['Low', 'High'] + [col for col in final_df.columns if col not in ['Low', 'High']]]\n",
    "\n",
    "# Add the percentage difference column to final_df\n",
    "final_df['Percentage Difference'] = ((final_df['High'] - final_df['Low']) / final_df['Low']) * 100"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdf853b5-ba78-4239-bbe6-337fa4c8d167",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the rest of your code is defined above and final_df is already created\n",
    "\n",
    "# Get today's date using pd.Timestamp and define the end date as today + 5 days\n",
    "today = pd.Timestamp('today').normalize()  # normalize to remove time component\n",
    "end_date = today + pd.Timedelta(days=5)\n",
    "\n",
    "# Filter final_df for rows where the index (Date) is between today and the next 5 days\n",
    "filtered_df = final_df.loc[today:end_date]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
